{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importamos las librerias necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import gzip\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n"
      ],
      "metadata": {
        "id": "fPzIQcREIZPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funciones auxiliares\n",
        "\n",
        "def crear_directorios_base():\n",
        "    \"\"\"Asegura que los directorios de salida existan.\"\"\"\n",
        "    os.makedirs(\"../files/models\", exist_ok=True)\n",
        "    os.makedirs(\"../files/output\", exist_ok=True)\n",
        "\n",
        "def cargar_datos_fuente():\n",
        "    \"\"\"Carga los DataFrames de entrenamiento y prueba.\"\"\"\n",
        "    train_df = pd.read_csv(\"../files/input/train_data.csv.zip\", compression=\"zip\")\n",
        "    test_df = pd.read_csv(\"../files/input/test_data.csv.zip\", compression=\"zip\")\n",
        "    return train_df, test_df\n",
        "\n",
        "def guardar_modelo_comprimido(estimator, path=\"../files/models/model.pkl.gz\"):\n",
        "    \"\"\"Guarda el estimador comprimido con gzip.\"\"\"\n",
        "    crear_directorios_base()\n",
        "    with gzip.open(path, \"wb\") as f:\n",
        "        pickle.dump(estimator, f)\n",
        "\n",
        "def recuperar_modelo(path=\"../files/models/model.pkl.gz\"):\n",
        "    \"\"\"Carga el estimador guardado si existe, sino devuelve None.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with gzip.open(path, \"rb\") as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "rWdrvaYVIvjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funciones para cargar y limpiar datos\n",
        "\n",
        "def limpiar_dataframe(df):\n",
        "    \"\"\"\n",
        "    Limpia y transforma un DataFrame:\n",
        "    Crea 'Age', elimina 'Year' y 'Car_Name'.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Creación de característica 'Age'\n",
        "    # Asumiendo 2021 como el año base\n",
        "    df[\"Age\"] = 2021 - df[\"Year\"]\n",
        "\n",
        "    # Eliminación de columnas no deseadas\n",
        "    df = df.drop([\"Year\", \"Car_Name\"], axis=1, errors='ignore')\n",
        "\n",
        "    # Eliminar NaNs\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "d9U68vRfI7kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# division\n",
        "\n",
        "def obtener_splits_entrenamiento_prueba(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Aplica limpiar_dataframe y retorna x_train, y_train, x_test, y_test\n",
        "    El target es 'Present_Price'.\n",
        "    \"\"\"\n",
        "    train_clean = limpiar_dataframe(train_df)\n",
        "    test_clean = limpiar_dataframe(test_df)\n",
        "\n",
        "    # Separación de variables\n",
        "    x_train = train_clean.drop(\"Present_Price\", axis=1)\n",
        "    y_train = train_clean[\"Present_Price\"]\n",
        "\n",
        "    x_test = test_clean.drop(\"Present_Price\", axis=1)\n",
        "    y_test = test_clean[\"Present_Price\"]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n"
      ],
      "metadata": {
        "id": "50GD0BbgI_pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funciones para el modelo\n",
        "\n",
        "def construir_pipeline_completo(feature_columns):\n",
        "    \"\"\"\n",
        "    Construye el pipeline para Regresión Lineal.\n",
        "    \"\"\"\n",
        "    # Definición de columnas\n",
        "    categorical_features = [\"Fuel_Type\", \"Selling_type\", \"Transmission\"]\n",
        "    numeric_features = [col for col in feature_columns if col not in categorical_features]\n",
        "\n",
        "    # Preprocesador\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            # Las numéricas se escalan con MinMaxScaler\n",
        "            (\"num\", MinMaxScaler(), numeric_features),\n",
        "            # Las categóricas se codifican con OHE\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "        ],\n",
        "\n",
        "        remainder=MinMaxScaler(),\n",
        "    )\n",
        "\n",
        "    # Armado del pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        # Función de puntuación para Regresión\n",
        "        (\"feature_selection\", SelectKBest(score_func=f_regression)),\n",
        "        (\"model\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def configurar_busqueda_grid(estimator, param_grid, cv=10):\n",
        "    \"\"\"Crea y configura el objeto GridSearchCV.\"\"\"\n",
        "    # scoring='neg_mean_absolute_error' y cv=10\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        refit=True\n",
        "    )\n",
        "    return grid_search"
      ],
      "metadata": {
        "id": "35KpbEdIJKf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones de Entrenamiento y Validación\n",
        "\n",
        "def entrenar_y_comparar_modelos(grid_search):\n",
        "\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    # Entrenar el modelo (GridSearchCV)\n",
        "    grid_search.fit(x_train, y_train)\n",
        "\n",
        "    # Guardar el mejor estimador\n",
        "    guardar_modelo_comprimido(grid_search)\n",
        "\n",
        "\n",
        "def ejecutar_entrenamiento_mlp():\n",
        "\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    pipeline = construir_pipeline_completo(feature_columns=x_train.columns.tolist())\n",
        "\n",
        "    param_grid = {\"feature_selection__k\": list(range(1, 20))}\n",
        "\n",
        "    gs = configurar_busqueda_grid(estimator=pipeline, param_grid=param_grid, cv=10)\n",
        "    entrenar_y_comparar_modelos(gs)\n",
        "\n",
        "\n",
        "def validar_y_generar_metricas():\n",
        "    \"\"\"\n",
        "    Carga el modelo final, calcula métricas de Regresión (R2, MSE, MAD)\n",
        "    y las guarda en un archivo JSONL.\n",
        "    \"\"\"\n",
        "    crear_directorios_base()\n",
        "    train_df, test_df = cargar_datos_fuente()\n",
        "    x_train, y_train, x_test, y_test = obtener_splits_entrenamiento_prueba(train_df, test_df)\n",
        "\n",
        "    # cargar modelo (gzip)\n",
        "    estimator = recuperar_modelo()\n",
        "    if estimator is None:\n",
        "        raise FileNotFoundError(\"No se encontró modelo en files/models/model.pkl.gz\")\n",
        "\n",
        "    # predicciones\n",
        "    y_train_pred = estimator.predict(x_train)\n",
        "    y_test_pred = estimator.predict(x_test)\n",
        "\n",
        "    metrics = []\n",
        "\n",
        "    # Métricas de entrenamiento (Regresión)\n",
        "    train_metrics = {\n",
        "        \"type\": \"metrics\",\n",
        "        \"dataset\": \"train\",\n",
        "        \"r2\": float(r2_score(y_train, y_train_pred)),\n",
        "        \"mse\": float(mean_squared_error(y_train, y_train_pred)),\n",
        "        \"mad\": float(median_absolute_error(y_train, y_train_pred)),\n",
        "    }\n",
        "    metrics.append(train_metrics)\n",
        "\n",
        "    # Métricas de prueba (Regresión)\n",
        "    test_metrics = {\n",
        "        \"type\": \"metrics\",\n",
        "        \"dataset\": \"test\",\n",
        "        \"r2\": float(r2_score(y_test, y_test_pred)),\n",
        "        \"mse\": float(mean_squared_error(y_test, y_test_pred)),\n",
        "        \"mad\": float(median_absolute_error(y_test, y_test_pred)),\n",
        "    }\n",
        "    metrics.append(test_metrics)\n",
        "\n",
        "    # guardar JSONL\n",
        "    out_path = \"../files/output/metrics.json\"\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for m in metrics:\n",
        "            f.write(json.dumps(m) + \"\\n\")\n",
        "\n",
        "    print(f\"Métricas guardadas en {out_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # si se ejecuta el script, entrena y luego comprueba\n",
        "    crear_directorios_base()\n",
        "    ejecutar_entrenamiento_mlp()\n",
        "    validar_y_generar_metricas()"
      ],
      "metadata": {
        "id": "WsMZeLSjIWj0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}